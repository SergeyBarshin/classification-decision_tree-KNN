{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors Method\n",
    "knn (k-Nearest Neighbors) - популярный метод машинного обучения используемый в задачах классификации и иногда при регрессии.\n",
    "Интуитивно суть метода такова: посмотри на соседей, какие преобладают, таков и ты.\n",
    "\n",
    "Основой метода является гипотеза компактности: если метрика расстояния между примерами введена достаточно удачно, то схожие примеры гораздочаще лежат в одном классе, чем в разных.\n",
    "\n",
    "Для классификации каждого объекта тестовой выборки, нужно выполнить следующие операции:\n",
    "1) Вычисляем расстояние до каждого из объектов обучаюещей выборки\n",
    "2) Отобрать $k$ объектов обучающей выборки, расстояние до которых минимально\n",
    "3) Класс классифицируемого объекта - это класс наиболее часто встречающийся у k ближайщих соседей\n",
    "\n",
    "В случае регрессии, на третьем шаге, мы берем не класс, а число среднее или медиана, целивого признака.\n",
    "\n",
    "Примечательно, что вычисления начинаются, только в момент крассификации, и никакое предобучение не проводится, в отличии от деревьев решения, где сначала проходит обучение, а потом быстро классифицирует тестовые примеры.\n",
    "\n",
    "Для knn существует не мало теорем утверждающих, что это лучший метод на бесконечных выборках. Knn теоритически идеальный алгоритм, его проблема"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применение knn\n",
    "- хороший безлайн для многих соревнования\n",
    "- в Kaggle, knn часто используется для конструирования мета признаков (прогноз knn подается на вход другим моделям) или для стекинга\\блидинга\n",
    "- идеи knn используются в рекомедательных системах\n",
    "- на практике, при больших датасетах, используют приближенные методы поиска соседей. \n",
    "\n",
    "**Качество** классификации\\регресиии метожом ближайших соседей зависит от нескольких параметров:\n",
    "- число соседей\n",
    "- метрика расстояния (хэмминга, евклидово, косисное). Так же значения признаков стоит масштабировать.\n",
    "- веса соседей. Чем дальше, тем меньше импакта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier в sklearn\n",
    "[sklearn.neighbors.KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "- `weights`: \"uniform\" (все веса равны), или \"distance\" (вес обратно пропорционален расстоянию до тестового примера) или другая определенная пользователем ф-ция\n",
    "- `algorithm`:  \"brute\", \"ball_tree\", \"KD_tree\", или \"auto\". В первом случае ближайшие соседи для каждого тестового примера считаются перебором обучающей выборки. Во втором и третьем — расстояние между примерами хранятся в дереве, что ускоряет нахождение ближайших соседей. В случае указания параметра \"auto\" подходящий способ нахождения соседей будет выбран автоматически на основе обучающей выборки.\n",
    "- `leaf_size` (опционально): порог переключения на полный перебор в случае выбора BallTree или KDTree для нахождения соседей\n",
    "- `metric`: \"minkowski\", \"manhattan\", \"euclidean\", \"chebyshev\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
