# Machine Learning - Decision Trees and KNN

Этот репозиторий посвящен изучению алгоритмов машинного обучения, а именно **деревьев решений** и **k ближайших соседей (KNN)**. Здесь представлены теоретические материалы и практические примеры реализации этих алгоритмов.

## Оглавление

- [Machine Learning - Decision Trees and KNN](#machine-learning---decision-trees-and-knn)
  - [Оглавление](#оглавление)
  - [Описание репозитория](#описание-репозитория)
    - [Текущие материалы:](#текущие-материалы)
    - [Заключение](#заключение)
      - [Деревья решений](#деревья-решений)
      - [Метод ближайщих соседей](#метод-ближайщих-соседей)

## Описание репозитория

Этот репозиторий создан для того, чтобы познакомиться с основами машинного обучения, начиная с деревьев решений и алгоритма KNN. На данный момент в репозитории есть только теоретический ноутбук, который объясняет принципы работы деревьев решений.

### Текущие материалы:

1. **Теория по деревьям решений:**  
   В данном ноутбуке подробно рассмотрены основные принципы построения деревьев решений, их использование в задачах классификации и регрессии, а также основные параметры, которые влияют на работу алгоритма.

[Ссылка на ноутбук с теорией](./Decision_Tree_theory.ipynb)

2. **Теория по knn**
   [Ссылка на ноутбук](./Nearest_Neighbors_method.ipynb)

3. **Кросс-валидация и подбор параметров**
   [Ссылка на ноутбук](./Cross_Validation.ipynb)

4. **MNIST**
   [Ссылка на ноутбук](./MNIST_knn_tree.ipynb)

### Заключение

#### Деревья решений

**Плюсы:**

- Генерируют четкие, понятные человеку правила классификации. Это свойство называют интерпретируемостью модели.
- Деревья лекго визуализировать. Сама модель и предсказание, легко интерпритируемы.
- Быстро тренируется и работает.
- Мало параметров.
- Поддерживает и численные и категоральные признаки.

**Минусы:**

- Очень чувствительны к шумам во входных данных, вся модель может кардинально измениться, если немного изменится обучающая выборка (например, если убрать один из признаков или добавить несколько объектов), поэтому и правила классификации могут сильно изменяться, что ухудшает интерпретируемость модели;
- Необходимость отсекать ветви дерева (pruning) или устанавливать минимальное число элементов в листьях дерева или максимальную глубину дерева для борьбы с переобучением.
- Модель умеет только интерполировать, но не экстраполировать (это же верно и для леса и бустинга на деревьях). То есть дерево решений делает константный прогноз для объектов, находящихся в признаковом пространстве вне параллелепипеда, охватывающего все объекты обучающей выборки. В нашем примере с желтыми и синими шариками это значит, что модель дает одинаковый прогноз для всех шариков с координатой > 19 или < 0.
- Проблема поиска оптимального дерева решений (минимального по размеру и способного без ошибок классифицировать выборку) NP-полна, поэтому на практике используются эвристики.

#### Метод ближайщих соседей

**Плюсы:**

- Простая реализация, сильно изучен теоритически.
- Хорош для построения бейзлайна, причем не только классификации и регрессии, но для рекомендательных систем.
- Можно адаптировать под нужную задачу выбором метрики или ядра (ядро может задавать операцию сходства для сложных объектов типа графов, а сам подход kNN остается тем же).
- Неплохая интерпретация, можно объяснить, почему тестовый пример был классифицирован именно так.

**Минусы:**

- Считается быстрым в сравнении, например с композицией объектов. Но в реальных задачах число соседей бывает велико (100-150), в таком случае будет работать не так быстро как деревья.
- Если в наборе данных много признаков, то трудно подобрать подходящие веса и определить, какие признаки не важны для классификации/регрессии.
- Зависимость от выбранной метрики расстояния между примерами. Выбор по умолчанию евклидового расстояния чаще всего ничем не обоснован. Можно отыскать хорошее решение перебором параметров, но для большого набора данных это отнимает много времени.
- Нет теоритического обоснования выбора, того или иного числа соседей. Только перебор. При малом числе соседей, алгорим склонен к переобучению.
- Как правило, плохо работает, когда признаков много, из-за "прояклятия размерности".
